[{"/Users/sandranissim/Desktop/Fall2020/DeepLearning/finalProject/dl-project/src/index.js":"1","/Users/sandranissim/Desktop/Fall2020/DeepLearning/finalProject/dl-project/src/App.js":"2","/Users/sandranissim/Desktop/Fall2020/DeepLearning/finalProject/dl-project/src/reportWebVitals.js":"3"},{"size":500,"mtime":1606865275844,"results":"4","hashOfConfig":"5"},{"size":8137,"mtime":1607575801573,"results":"6","hashOfConfig":"5"},{"size":362,"mtime":1606865275846,"results":"7","hashOfConfig":"5"},{"filePath":"8","messages":"9","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"10"},"110ffym",{"filePath":"11","messages":"12","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"13"},{"filePath":"14","messages":"15","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"10"},"/Users/sandranissim/Desktop/Fall2020/DeepLearning/finalProject/dl-project/src/index.js",[],["16","17"],"/Users/sandranissim/Desktop/Fall2020/DeepLearning/finalProject/dl-project/src/App.js",["18"],"import logo from './logo.svg';\nimport './App.css';\n\n\nconst headerStyle = {\n  backgroundColor: \"#3B6275\",\n  padding: 40\n};\n\nconst titleStyle = {\n  color: \"#ADCEDE\",\n  fontSize: \"36px\",\n}\n\nconst subStyle = {\n  color: \"#1B2C36\",\n  fontSize: \"18px\",\n  lineHeight: .6,\n  padding: 30,\n}\n\nconst leftStyle = {\n  float: \"left\",\n  width: \"40%\",\n  paddingLeft: \"5%\"\n}\n\nconst rightStyle = {\n  float: \"right\",\n  width: \"40%\",\n  paddingRight: \"5%\"\n}\n\nconst courseStyle = {\n  paddingTop: 100,\n}\n\nconst aboutStyle = {\n  color: \"#61A2C2\",\n  float: \"left\",\n  fontSize: \"32px\",\n  paddingLeft: \"5%\",\n  paddingTop: 30,\n}\n\nconst contentStyle = {\n  color: \"#1B2C36\",\n  float: \"left\",\n  fontSize: \"18px\",\n  width: \"90%\",\n  textAlign: \"left\",\n  paddingLeft: \"5%\",\n}\n\nconst linkStyle = {\n  color: \"#61A2C2\",\n}\n\nconst imageStyle = {\n  justifyContent: \"center\",\n}\n\nconst figureLeftStyle = {\n  float: \"left\",\n  width: \"40%\",\n}\n\nconst figureRightStyle = {\n  float: \"right\",\n  width: \"40%\",\n}\n\nfunction App() {\n  return (\n    <div className=\"App\">\n      <header style={headerStyle}>\n        <p style={titleStyle}>Analyzing the Classification of Skin Lesions Through Convolutional Neural Networks</p>\n      </header>\n\n      <body>\n        <div style={subStyle}>\n          <div style={leftStyle}>\n            <p><b>Jordan Zax</b></p>\n            <p>jordanzax2021@u.northwestern.edu</p>\n          </div>\n          <div style={rightStyle}>\n            <p><b>Sandra Nissim</b></p>\n            <p>sandranissim2021@u.northwestern.edu</p>\n          </div>\n          <div style={courseStyle}>\n            <p>Professor Bryan Pardo</p>\n            <p>Computer Science 396/496: Deep Learning</p>\n            <p>Northwestern University</p>\n          </div>\n\n        </div>\n\n        <div>\n          <div style={aboutStyle}>\n            <p><em><b>About</b></em></p>\n          </div>\n          <div style={contentStyle}>\n            <p>\n              We have made a skin cancer diagnosis tool that classifies different skin lesions \n              as specific types of malignant and benign cancers (i.e. melanoma, moles, warts, \n              etc.). Specifically, we classify between melanoma, melanocytic nevi, dermatofibroma, \n              actinic keratoses, vascular lesions (i.e. angiomas, angiokeratomas, pyogenic granulomas, \n              hemorrhage), squamous cell carcinoma, pigmented benign keratoses (i.e. noncancerous UV \n              damage), and basal cell carcinoma. This is an interesting problem because skin cancer is \n              one of the most prevalent types of cancer. According to the World Health Organization, \n              one in every three cancers diagnosed each year is skin cancer and one in five people are \n              diagnosed throughout their life. An early diagnosis has a 98% survival rate; once the cancer \n              reaches the lymphatics the survival rate drops to 62% and once metastasized, down to a mere \n              16%. As such, skin cancer is a deadly disease which impacts many people. Lesions are easy to \n              ignore as they seem trivial; having such a classification tool could drastically assist in \n              catching skin cancers early and thus improve survival rates. The goal of this model is to \n              classify various lesions as a means of helping a user to determine whether any given lesion \n              is malignant or benign. \n            </p>\n            <p>\n              The work presented in this paper primarily focused on two convolutional neural networks: \n              AlexNet and ResNet152. We analyzed and compared the results from the models in \n              terms of accuracy and efficiency. Each neural network took in a 224x264 pixel photo as input; \n              the models output a classification from those aforementioned. In the experimentation phase, \n              we created models with various optimization criterion and losses in addition to various \n              parameters as an effort to finding the most successful results. Ultimately, we chose cross \n              entropy loss and stochastic gradient descent for the majority of our testing. Furthermore, \n              we found most success with a learning rate of 0.001, weight decay of 0.00001, and momentum \n              of 0.9. Our models ran with a batch size of 8 for a total of 100 epochs.\n            </p>\n            <p>\n              In order to train the models, we used the HAM10000 dataset in addition to the GCN dataset, both \n              of which are accessible via the ISIC database, for a total of 22,000 examples. In addition to \n              combining various datasets, we also augmented the inputted images to the model to provide a more \n              diverse dataset. Each image was randomly horizontally flipped with a probability of 0.5, randomly \n              vertically flipped with a probability of 0.5, and randomly cropped to ensure that lesions are found \n              in all locations of an image rather than being concentrated in the center. The images were also \n              normalized upon input. The data was split into training, validation, and testing sets randomly. \n              After training finished, we analyzed the accuracy and loss of the training and validation sets of \n              data plotted against epochs and the overall accuracy and loss of the test set of data. These performance \n              measures were executed twice for each model, once using a top 1 error rate, the traditional rate of \n              misclassification, in addition to broader measures representing the misclassification between benign \n              and malignant lesions (i.e. if a lesion is classified as the wrong type of lesion but it is correctly \n              classified as a malignant or benign type, it will not contribute to the error).\n            </p>\n            <p>\n              In our experiment analyzation process, we found ResNet152 to be much more efficient and accurate than \n              the AlexNet. As such, we further focused on hyper-parameter tuning for the ResNet152 based model. Though\n              the AlexNet reached an accuracy of approximately 929924 for the full 8-way classification (i.e. classifying \n              specific types of lesions rather than just benign/malignant), the ResNet152 model for the same type of \n              8-way classification reached an accuracy of 0.795 with loss of 0.56. When analyzing the \n              2-way classification of the same ResNet152 model, we noted an accuracy of 0.84 with loss of 0.285. As is \n              evident in the figures provided, a bit of overfitting occurred towards the end of the 100 epoch runs. \n              Regardless, dermatologists have a diagnostic accuracy of approximately 60%. As such, the results of this \n              model are significant as the model successfully classifies with accuracy significantly higher. \n          </p>\n          </div>\n\n          <div style={imageStyle}>\n            <figure style={figureLeftStyle}>\n              <img src=\"https://drive.google.com/uc?export=view&id=19W1W50OQTQiZfHV2UA0OXc4_Q0nBd5v1\" width=\"428\" height=\"282\" alt=\"\"></img>\n              <figcaption>Figure 1: ResNet152 Benign/Malignant 2 Way Classification Accuracy.</figcaption>\n            </figure>\n            <figure style={figureRightStyle}>\n              \n              <img src=\"https://drive.google.com/uc?export=view&id=1T6beGb4JVDuWgQ_BMdgi1bode2CuGN9d\" width=\"428\" height=\"282\" alt=\"\"></img>\n              <figcaption>Figure 2: ResNet152 8 Way Classification Accuracy.</figcaption>\n            </figure>\n          </div>\n        </div>\n\n        <div>\n          <div style={aboutStyle}>\n            <p><em><b>Research Paper</b></em></p>\n          </div>\n        </div>\n        <div style={contentStyle}>\n          <p>The full abstract detailing the work done is accessible \n            <a style={linkStyle} href=\"https://external-preview.redd.it/TsqzbXm1bq59S-MvrzDoqxdvUEA9tuTg0q-uhFezXeA.jpg?auto=webp&s=e2773da412916c059f56d47c916b62d92fca4198\"> here.</a>\n          </p>\n        </div>\n      </body>\n    </div>\n  );\n}\n\n\nexport default App;\n","/Users/sandranissim/Desktop/Fall2020/DeepLearning/finalProject/dl-project/src/reportWebVitals.js",[],{"ruleId":"19","replacedBy":"20"},{"ruleId":"21","replacedBy":"22"},{"ruleId":"23","severity":1,"message":"24","line":1,"column":8,"nodeType":"25","messageId":"26","endLine":1,"endColumn":12},"no-native-reassign",["27"],"no-negated-in-lhs",["28"],"no-unused-vars","'logo' is defined but never used.","Identifier","unusedVar","no-global-assign","no-unsafe-negation"]